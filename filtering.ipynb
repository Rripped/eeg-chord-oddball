{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducing and Filtering of Chord-Oddball data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.insert(1, \"../\")\n",
    "import ccs_eeg_utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne.preprocessing as prep\n",
    "import os\n",
    "import sklearn \n",
    "from contextlib import contextmanager\n",
    "from autoreject import AutoReject, get_rejection_threshold\n",
    "import json\n",
    "\n",
    "from mne_bids import (BIDSPath, read_raw_bids, write_raw_bids, inspect_dataset)\n",
    "import auc\n",
    "\n",
    "matplotlib.use('Qt5Agg')\n",
    "\n",
    "%matplotlib qt\n",
    "\n",
    "# path where dataset is stored\n",
    "bids_root = \"./data/ds003570/\"\n",
    "TASK = 'AuditoryOddballChords'\n",
    "SUBJECT = '002'\n",
    "SUPRESS_BIDS_OUTPUT = True\n",
    "PROMPT_BADS = False\n",
    "USE_ICA_JSON = False\n",
    "Z_SCORE_REJECT = False\n",
    "AUTOREJECT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context manager to suppress stdout and stderr\n",
    "@contextmanager\n",
    "def suppress_stdout_stderr():\n",
    "    with open(os.devnull, 'w') as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        old_stderr = sys.stderr\n",
    "        sys.stdout = devnull\n",
    "        sys.stderr = devnull\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout\n",
    "            sys.stderr = old_stderr\n",
    "\n",
    "def read_raw_data(subject_id):\n",
    "    bids_path = BIDSPath(subject=subject_id,\n",
    "                         datatype='eeg', suffix='eeg', task=TASK,\n",
    "                         root=bids_root)\n",
    "\n",
    "    if SUPRESS_BIDS_OUTPUT:\n",
    "        with suppress_stdout_stderr():\n",
    "            raw = read_raw_bids(bids_path)\n",
    "    else:\n",
    "        raw = read_raw_bids(bids_path)\n",
    "\n",
    "    # Inplace?\n",
    "    ccs_eeg_utils.read_annotations_core(bids_path, raw)\n",
    "    raw.load_data()\n",
    "    \n",
    "    return raw, bids_path\n",
    "\n",
    "def preprocessing(raw):\n",
    "    # TODO: bandpass first, downsample later? -> expensive!\n",
    "    # 1. Downsampling to 128 Hz\n",
    "    if raw.info['sfreq'] > 128:\n",
    "        raw.resample(128)\n",
    "\n",
    "    # Set channel types to EEG if not already set\n",
    "    if not all(ch_type in ['eeg', 'stim'] for ch_type in raw.get_channel_types()):\n",
    "        eeg_channel_names = raw.ch_names\n",
    "        channel_types = {name: 'eeg' for name in eeg_channel_names}\n",
    "        raw.set_channel_types(channel_types)\n",
    "\n",
    "    # 2. Band-pass filter between 0.5 Hz and 30 Hz\n",
    "    raw.filter(0.5, 30, fir_design='firwin')\n",
    "\n",
    "    # 3. Re-referencing to the average activity of all electrodes\n",
    "    #TODO: add apply_proj() here to apply arp?\n",
    "    raw.set_eeg_reference('average', projection=True)\n",
    "\n",
    "    \"\"\" events = prep.find_eog_events(raw)\n",
    "    print(events) \"\"\"\n",
    "\n",
    "    # 5. Data Reduction (optional)\n",
    "    # For instance, crop the first 60 seconds of the data\n",
    "\n",
    "    return raw\n",
    "\n",
    "\n",
    "def save_preprocessed_data(file_path, raw):\n",
    "    \"\"\"\n",
    "    Saves the preprocessed EEG data to a file.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): The path where the preprocessed data will be saved.\n",
    "    raw (mne.io.Raw): The preprocessed MNE Raw object containing EEG data.\n",
    "    \"\"\"\n",
    "    # Check if file_path ends with .fif extension\n",
    "    if not file_path.endswith('.fif'):\n",
    "        file_path += '.fif'\n",
    "\n",
    "    # Save the data\n",
    "    try:\n",
    "        raw.save(file_path, overwrite=True)\n",
    "        print(f\"Data saved successfully to {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving data: {e}\")\n",
    "\n",
    "# see https://neuraldatascience.io/7-eeg/erp_artifacts.html\n",
    "def get_ica(data, ica_bads, block_idx):\n",
    "    data.set_montage('standard_1020', match_case=False)\n",
    "    ica = mne.preprocessing.ICA(method=\"fastica\", random_state=0)\n",
    "  \n",
    "    ica.fit(data, verbose=True)\n",
    "\n",
    "    if USE_ICA_JSON:\n",
    "        exclude_components = ica_bads[f\"sub-{SUBJECT}\"][block_idx]\n",
    "    else:\n",
    "        ica_n_components = ica.n_components_\n",
    "        n_components = 64\n",
    "        fig, axes = plt.subplots(nrows=7, ncols=10, figsize=(15, 10))\n",
    "        axes = axes.flatten()\n",
    "\n",
    "        for i, ax in enumerate(axes[:ica_n_components]):\n",
    "            if i < n_components:\n",
    "                ica.plot_components(picks=i, show=False, axes=ax)\n",
    "            else:\n",
    "                ax.set_visible(False)\n",
    "        plt.show(block=True)\n",
    "\n",
    "        # ica.plot_components()\n",
    "        # ica.plot_properties(data)\n",
    "        # plt.show(block=True)\n",
    "        # components to be excluded\n",
    "        # add python input to determine which components to exclude\n",
    "        input_str = input(\"Enter index of the components to be separated by space: \")  \n",
    "        # Converting input string to a list of integers\n",
    "    \n",
    "        exclude_components = input_str.split()  \n",
    "        exclude_components = [int(num) for num in exclude_components]  \n",
    "        \n",
    "        ica_bads[f\"sub-{SUBJECT}\"][block_idx] = exclude_components\n",
    "\n",
    "    # Printing the list  \n",
    "    print(\"List of components:\", exclude_components) \n",
    "    ica.exclude = exclude_components\n",
    "    reconst_raw = data.copy()\n",
    "    # apply with excluded components\n",
    "    reconst_raw = ica.apply(reconst_raw)\n",
    "    \n",
    "    return reconst_raw\n",
    "\n",
    "\n",
    "def split_in_blocks(raw):\n",
    "    events, event_id = mne.events_from_annotations(raw)\n",
    "\n",
    "    # Identify indices of 'STATUS:boundary' events\n",
    "    boundary_events = events[events[:, 2] == event_id['STATUS:boundary'], 0]\n",
    "\n",
    "    # Split the data into blocks\n",
    "    blocks = []\n",
    "    start_idx = 0\n",
    "    for end_idx in boundary_events:\n",
    "        block = raw.copy().crop(tmin=raw.times[start_idx], tmax=raw.times[end_idx])\n",
    "        blocks.append(block)\n",
    "        start_idx = end_idx\n",
    "\n",
    "    block = raw.copy().crop(tmin=raw.times[start_idx])\n",
    "    blocks.append(block)    \n",
    "\n",
    "    return blocks\n",
    "\n",
    "\n",
    "def mark_bad_channels_by_z_score(raw_data, threshold=8.0):\n",
    "    \"\"\"\n",
    "    Identify bad channels in raw data based on amplitude.\n",
    "    Channels with z-score > threshold are marked as bad.\n",
    "\n",
    "    Parameters:\n",
    "    raw_data (mne.io.Raw): The raw data.\n",
    "    threshold (float): The z-score threshold to use.\n",
    "\n",
    "    Returns:\n",
    "    list: List of bad channels.\n",
    "    \"\"\"\n",
    "    data, times = raw_data[:, :]\n",
    "    channel_means = np.mean(data, axis=1)\n",
    "    channel_stds = np.std(data, axis=1)\n",
    "    z_scores = np.abs((data - channel_means[:, None]) / channel_stds[:, None])\n",
    "    bad_channels = np.where(np.any(z_scores > threshold, axis=1))[0]\n",
    "    return [raw_data.ch_names[i] for i in bad_channels]\n",
    "\n",
    "\n",
    "def get_epochs_from_events(data, event_str, min_reaction_s=None):\n",
    "    evts, evts_dict = mne.events_from_annotations(data)\n",
    "\n",
    "    # Identify deviant events\n",
    "    deviant_keys = [e for e in evts_dict.keys() if e.endswith(event_str)]\n",
    "    correct_keys = [e for e in evts_dict.keys() if \"STATUS:Correct - \" in e]\n",
    "\n",
    "    # Construct a dictionary of deviant events\n",
    "    evts_dict_stim = {}\n",
    "    for key in deviant_keys:\n",
    "        evts_dict_stim[key] = evts_dict[key]\n",
    "\n",
    "    data.info.normalize_proj()\n",
    "\n",
    "    epochs = mne.Epochs(data, evts, evts_dict_stim, tmin=-0.4, tmax=1.6, baseline=(-0.4, 0), preload=True)\n",
    "\n",
    "    if min_reaction_s:\n",
    "        # Calculate reaction times\n",
    "        d_evts = evts[evts[:, 2] == evts_dict_stim[deviant_keys[0]]]\n",
    "\n",
    "        reaction_times = []\n",
    "        for key in deviant_keys[1:]:            \n",
    "            d_evts = np.concatenate((d_evts, evts[evts[:, 2] == evts_dict_stim[key]]))\n",
    "        \n",
    "        # Sort events by time, to fit indexes to the indices in epochs\n",
    "        d_evts = sorted(d_evts, key=lambda x: x[0])\n",
    "\n",
    "        i = 0\n",
    "        for d_evt in d_evts:\n",
    "            # Find the closest \"Correct -\" event\n",
    "            c_evts = evts[np.isin(evts[:, 2], [evts_dict[key] for key in correct_keys]) & (evts[:, 0] > d_evt[0])]\n",
    "            if len(c_evts) > 0:\n",
    "                # Calculate the reaction time\n",
    "                reaction_time = (c_evts[0][0] - d_evt[0]) / data.info['sfreq']  # convert to ms\n",
    "                print(reaction_time)\n",
    "                reaction_times.append((reaction_time, key, i))\n",
    "                i += 1\n",
    "        \n",
    "        #Filter epochs based on reaction time\n",
    "        valid_epochs = [i for (rt, _, i) in reaction_times if min_reaction_s <= rt]\n",
    "        print(f\"Filtered {len(valid_epochs)} epochs out of {len(epochs)} based on reaction time threshold\")\n",
    "        epochs = epochs[valid_epochs]\n",
    "    \n",
    "\n",
    "    # Reject threshold\n",
    "    reject = dict(eeg=0.0004)  # in V\n",
    "    epochs.drop_bad(reject = reject)\n",
    "\n",
    "    return epochs\n",
    "\n",
    "\n",
    "def interpolate_bads_and_merge(blocks):\n",
    "    # Interpolate bad channels\n",
    "    for block in blocks:\n",
    "        block.interpolate_bads()\n",
    "\n",
    "    # Merge the blocks\n",
    "    raw = mne.concatenate_raws(blocks)\n",
    "\n",
    "    return raw\n",
    "\n",
    "\n",
    "def create_bad_json_structure():\n",
    "    subjects = {}\n",
    "    for s in range(1, 41):\n",
    "        subject_key = f'sub-{s:03d}'\n",
    "        subjects[subject_key] = {}\n",
    "        for b in range(1, 9):\n",
    "            block_key = f'{b}'\n",
    "            subjects[subject_key][block_key] = []\n",
    "\n",
    "    return subjects\n",
    "\n",
    "def set_bad_channels_from_json(blocks, bad_json):\n",
    "    for block in blocks:\n",
    "        # Set bad channels\n",
    "        block.info['bads'] = bad_json[f\"sub-{SUBJECT}\"][f\"{blocks.index(block)+1}\"]\n",
    "\n",
    "    return blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce bids eeg data\n",
    "if not os.path.isfile(f\"./data/processed_{SUBJECT}_raw.fif\"):\n",
    "    raw, bids_path = read_raw_data(SUBJECT)\n",
    "    channel_types = {ch: 'eeg' for ch in raw.ch_names}\n",
    "    raw.set_channel_types(channel_types)\n",
    "    \n",
    "    elec_data = pd.read_csv('./data/ds003570/sub-'+SUBJECT+'/eeg/sub-'+SUBJECT+'_task-AuditoryOddballChords_electrodes.tsv', sep='\\t')\n",
    "    montage = mne.channels.make_dig_montage(ch_pos=dict(zip(elec_data['name'], elec_data[['x', 'y', 'z']].values)),\n",
    "                                        coord_frame='head')\n",
    "    raw.set_montage(montage)\n",
    "    blocks = split_in_blocks(raw.copy())\n",
    "\n",
    "    if os.path.isfile(\"./data/bad_channels.json\"):\n",
    "        bads = json.load(open(\"./data/bad_channels.json\"))\n",
    "        blocks = set_bad_channels_from_json(blocks, bads)\n",
    "    else:\n",
    "        bads = create_bad_json_structure()\n",
    "\n",
    "    if os.path.isfile(\"./data/bad_ica_components.json\"):\n",
    "        ica_bads = json.load(open(\"./data/bad_ica_components.json\"))\n",
    "        blocks = set_bad_channels_from_json(blocks, bads)\n",
    "    else:\n",
    "        ica_bads = create_bad_json_structure()\n",
    "\n",
    "    if PROMPT_BADS == True:\n",
    "        for b in blocks:\n",
    "            b.plot(n_channels=64)\n",
    "            plt.show(block=True)\n",
    "            bads[f\"sub-{SUBJECT}\"][f\"{blocks.index(b)+1}\"] = b.info['bads']\n",
    "    if Z_SCORE_REJECT == True:\n",
    "        for b in blocks:\n",
    "            # reject by z-score (autoreject is more sophisticated, but only works on epochs)\n",
    "            b.info['bads'].extend(mark_bad_channels_by_z_score(b, threshold=4))\n",
    "            print(f\"Bad channels: {b.info['bads']}\")\n",
    "            # store? no, because determined by z-score threshold, no random component\n",
    "            # bads[f\"sub-{SUBJECT}\"][f\"{blocks.index(b)+1}\"] = b.info['bads']\n",
    "    \n",
    "    with open(\"./data/bad_channels.json\", \"w\") as f:\n",
    "        json.dump(bads, f)\n",
    "\n",
    "    ica_blocks = []\n",
    "\n",
    "    for b in blocks:\n",
    "        b.interpolate_bads()\n",
    "        prep_block = preprocessing(b.copy())\n",
    "\n",
    "        # ICA\n",
    "        #ica_block = prep_block\n",
    "        ica_block = get_ica(prep_block, ica_bads, f\"{blocks.index(b)+1}\")\n",
    "        ica_blocks.append(ica_block)\n",
    "    \n",
    "    with open(\"./data/bad_ica_components.json\", \"w\") as f:\n",
    "        json.dump(ica_bads, f)\n",
    "\n",
    "    prep_raw = mne.concatenate_raws(ica_blocks)\n",
    "    save_preprocessed_data(f\"./data/processed_{SUBJECT}_raw.fif\", prep_raw)\n",
    "\n",
    "else:\n",
    "    prep_raw = mne.io.read_raw_fif(f\"./data/processed_{SUBJECT}_raw.fif\", preload=True)\n",
    "\n",
    "prep_raw.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_subselect = prep_raw.copy()\n",
    "raw_subselect.annotations\n",
    "\n",
    "standard_epochs = get_epochs_from_events(raw_subselect, '_S')\n",
    "exemplar_epochs = get_epochs_from_events(raw_subselect, '_deviantEcorrect_E', min_reaction_s=0.2)\n",
    "function_epochs = get_epochs_from_events(raw_subselect, '_deviantFcorrect_F', min_reaction_s=0.2)\n",
    "\n",
    "def apply_autoreject_info(epochs, autoreject_info):\n",
    "    \"\"\"applies the rejection thresholds and marks bad epochs from stored data\"\"\"\n",
    "    threshes = autoreject_info['threshes']\n",
    "    reject_log = autoreject_info['reject_log']\n",
    "\n",
    "    # apply learned thresholds\n",
    "    epochs.drop_bad(reject=threshes)\n",
    "\n",
    "    # mark and drop bad epochs\n",
    "    for i, is_bad in enumerate(reject_log):\n",
    "        if is_bad:\n",
    "            epochs.drop(i)\n",
    "\n",
    "    return epochs\n",
    "\n",
    "autoreject_file = f\"./data/autoreject_info_{SUBJECT}.json\"\n",
    "\n",
    "if AUTOREJECT:\n",
    "    if os.path.exists(autoreject_file):\n",
    "        with open(autoreject_file, 'r') as f:\n",
    "            autoreject_info = json.load(f)\n",
    "\n",
    "        # apply autoreject results to save time\n",
    "        standard_epochs_clean = apply_autoreject_info(standard_epochs, autoreject_info['standard'])\n",
    "        exemplar_epochs_clean = apply_autoreject_info(exemplar_epochs, autoreject_info['exemplar'])\n",
    "        function_epochs_clean = apply_autoreject_info(function_epochs, autoreject_info['function'])\n",
    "\n",
    "    else:\n",
    "        ar_standard = AutoReject()\n",
    "        ar_exemplar = AutoReject()\n",
    "        ar_function = AutoReject()\n",
    "\n",
    "        # apply autoreject (takes 3-6 min)\n",
    "        autoreject_info = {}\n",
    "\n",
    "        standard_epochs_clean, reject_log_standard = ar_standard.fit_transform(standard_epochs, return_log=True)\n",
    "        autoreject_info['standard'] = {\n",
    "            'bad_epochs': reject_log_standard.bad_epochs,\n",
    "            'reject_log': reject_log_standard.labels.tolist(),\n",
    "            'threshes': ar_standard.get_reject_log(standard_epochs).threshes_\n",
    "        }\n",
    "\n",
    "        exemplar_epochs_clean, reject_log_exemplar = ar_exemplar.fit_transform(exemplar_epochs, return_log=True)\n",
    "        autoreject_info['exemplar'] = {\n",
    "            'bad_epochs': reject_log_exemplar.bad_epochs,\n",
    "            'reject_log': reject_log_exemplar.labels.tolist(),\n",
    "            'threshes': ar_exemplar.get_reject_log(exemplar_epochs).threshes_\n",
    "        }\n",
    "\n",
    "        function_epochs_clean, reject_log_function = ar_function.fit_transform(function_epochs, return_log=True)\n",
    "        autoreject_info['function'] = {\n",
    "            'bad_epochs': reject_log_function.bad_epochs,\n",
    "            'reject_log': reject_log_function.labels.tolist(),\n",
    "            'threshes': ar_function.get_reject_log(function_epochs).threshes_\n",
    "        }\n",
    "\n",
    "        # save to json per sub\n",
    "        with open(autoreject_file, \"w\") as f:\n",
    "            json.dump(autoreject_info, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_rejection(epochs, shape):\n",
    "    std_concat = np.std(epochs)\n",
    "\n",
    "    # preallocate memory for the rejected epochs\n",
    "    epochs_concat_removed = np.zeros(shape=shape)\n",
    "    idx = 0  # index to keep track of the position in the pre-allocated array\n",
    "\n",
    "    print(\"len prior: \", len(epochs))\n",
    "\n",
    "    for epoch in epochs:\n",
    "        for channel in epoch:\n",
    "            channel_max = np.max(np.abs(channel))\n",
    "            std_channel = np.std(channel)\n",
    "\n",
    "            # apply the rejection criteria\n",
    "            if channel_max < (5 * std_concat) and channel_max < (250 * 1e-6) and channel_max < (5 * std_channel):\n",
    "                if idx < shape[0]:  # check to prevent index out of bounds\n",
    "                    epochs_concat_removed[idx] = channel\n",
    "                    idx += 1\n",
    "\n",
    "    # truncate the array to the actual size\n",
    "    epochs_concat_removed = epochs_concat_removed[:idx]\n",
    "\n",
    "    print(\"len after: \", len(epochs_concat_removed))\n",
    "\n",
    "    return epochs_concat_removed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exemplar_epochs = epoch_rejection(exemplar_epochs.get_data(), np.array(exemplar_epochs.get_data()).shape)\n",
    "\n",
    "# function_epochs = epoch_rejection(function_epochs.get_data(), np.array(function_epochs.get_data()).shape)\n",
    "\n",
    "# standard_epochs = epoch_rejection(standard_epochs.get_data(), np.array(standard_epochs.get_data()).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 5\n",
    "window = 13\n",
    "\n",
    "roc_exemplar = auc.generate_AUC_ROC_sliding_window(standard_epochs_clean.get_data(), exemplar_epochs_clean.get_data(), window, step)\n",
    "roc_function = auc.generate_AUC_ROC_sliding_window(standard_epochs_clean.get_data(), function_epochs_clean.get_data(), window, step)\n",
    "time = [i*step/128 -0.4 for i in range(len(roc_exemplar))]\n",
    "\n",
    "plt.plot(time, roc_exemplar, label=\"exemplar\")\n",
    "plt.plot(time, roc_function, label=\"function\")\n",
    "plt.vlines(-0.4, label=\"Chord 1\")\n",
    "plt.vlines(0, label=\"Chord 2\")\n",
    "plt.vlines(0.4, label=\"Chord 3\")\n",
    "plt.ylabel(\"AUC-Value\")\n",
    "plt.xlabel(\"Time in seconds\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "np.savetxt(f\"./data/auc_roc_sl_13_5_{SUBJECT}.txt\", np.column_stack((roc_exemplar, roc_function)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roc_exemplar = generate_AUC_ROC_sliding_window(standard_epochs, exemplar_epochs, window, step)\n",
    "# roc_function = generate_AUC_ROC_sliding_window(standard_epochs, function_epochs, window, step)\n",
    "# time = [i*step/128 -0.4 for i in range(len(roc_exemplar))]\n",
    "\n",
    "# plt.plot(time, roc_exemplar, label=\"exemplar\")\n",
    "# plt.plot(time, roc_function, label=\"function\")\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
